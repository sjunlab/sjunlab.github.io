# Word2Vec를 활용한 부동산 기사 분석

벡터간 유의미한 유사도를 반영하여 단어의 의미를 수치화하는 워드투벡터를 사용하여 네이버 부동산 기사의 단어간 유사도를 분석하였다.

분석을 위해 네이버 기사의 부동산 관련 기사 1000건을 크롤링한 데이터를 사용하였다.

## 부동산 기사 데이터 로드

```python
import pandas as pd
import matplotlib.pyplot as plt
import urllib.request
from gensim.models.word2vec import Word2Vec
from konlpy.tag import Okt
from tqdm import tqdm
from time import sleep
import warnings
warnings.filterwarnings('ignore')

train_data = pd.read_csv('F:/과업/임베딩/네이버 부동산 기사 크롤링/네이버뉴스_부동산_2022-09-19_20시54분.txt',sep='|')
train_data.head(5)
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }

</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>url</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>천안시, 국토부에 부동산 '조정지역' 해제 강력 요청</td>
      <td>http://www.newsis.com/view/?id=NISX20220919_00...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>부동산 시장 침체·고환율 흐름에 외국인 국내 부동산 매수 “꿈틀”</td>
      <td>http://www.segyebiz.com/newsView/2022091951716...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>경기도, 부동산거래 거짓신고 연말까지 특별조사</td>
      <td>https://www.donga.com/news/article/all/2022091...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>충남도 “천안?공주?논산 부동산 조정지역 해제해달라”</td>
      <td>https://www.news1.kr/articles/4806325</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>부동산원, 초등학생 4~6학년 대상 녹색건축물 견학소감문 공모</td>
      <td>https://daily.hankooki.com/news/articleView.ht...</td>
    </tr>
  </tbody>
</table>
</div>

분석을 위한 전처리 과정으로 데이터의 결측값을 제거 후 확인하는 과정을 거친다.

```python
#null값 유무
print(train_data.isnull().values.any())

#결측값제거 후 결측값 확인
train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거
print(train_data.isnull().values.any())
```

    False
    False

정규표현식을 사용하여 한국어를 제외한 문자를 제거한다.

```python
#정규표현식을 통한 한국어 외 문자 제거
train_data['title'] = train_data['title'].str.replace("[^ㄱ-ㅎㅏ-ㅣ가-힣 ]","")
```

사용하지 않는 조사와 불용어들을 정의한다

```python
# 불용어 정의
stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']
```

형태소 분석기 OKT를 사용하여 토큰화 작업을 진행한다. 토큰화란 문법적으로 더이상 나눌 수 없는 언어적 요소로 문장을 분할한다는 뜻이다.

```python
# 형태소 분석기 OKT를 사용한 토큰화 작업 (다소 시간 소요)
okt = Okt()

tokenized_data = []
for sentence in tqdm(train_data['title']):
    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화
    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거
    tokenized_data.append(stopwords_removed_sentence)
```

    100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:10<00:00, 91.45it/s]

기사 제목의 최대 길이와 평균 길이를 확인하고 이를 차트화 하여 시각화 한다.

```python
# 제목 길이 분포 확인
print('제목의 최대 길이 :',max(len(review) for review in tokenized_data))
print('제목의 평균 길이 :',sum(map(len, tokenized_data))/len(tokenized_data))
plt.hist([len(review) for review in tokenized_data], bins=50)
plt.xlabel('length of samples')
plt.ylabel('number of samples')
plt.show(block=True)
```

![](../image/2022-09-20-word2vec/output_13_1.png)

    제목의 최대 길이 : 21
    제목의 평균 길이 : 9.895

워드 임베딩 모듈인 Word2Vec를 사용하여 토큰화 된 네이버 기사 데이터를 학습한다.

```python
#Word2Vec으로 토큰화 된 네이버 기사 데이터를 학습합니다.

from gensim.models import Word2Vec

model = Word2Vec(sentences = tokenized_data, vector_size = 100, window = 5, min_count = 5, workers = 4, sg = 0)
```

완성된 메트릭스의 크기를 확인한다.

```python
# 완성된 임베딩 매트릭스의 크기 확인
model.wv.vectors.shape
```

    (413, 100)

총 413개의 단어가 존재하며 각 단어는 100차원으로 구성되어져 있다. '부동산'과 유사한 단어들을 뽑는다.

```python
print(model.wv.most_similar("부동산"))
```

    [('년', 0.9992895126342773), ('아파트', 0.9992791414260864), ('시장', 0.9992291331291199), ('거래', 0.9992142915725708), ('대출', 0.9991016387939453), ('세', 0.9990447163581848), ('집값', 0.9988975524902344), ('사', 0.9988916516304016), ('건', 0.9988853335380554), ('위', 0.9988706707954407)]
